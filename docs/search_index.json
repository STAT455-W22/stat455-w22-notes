[["index.html", "Stat 455: Advanced Statistical Modeling Notes Preface", " Stat 455: Advanced Statistical Modeling Notes 2022-01-01 Preface These notes are written to accompany the text Beyond Multiple Linear Regression by Roback and Legler. Much of the text is either directly from the book, or lightly modified/summarized. Most of the code originates from the book's Github repository. These notes will guide our lectures and class discussion, but they are not sufficient as a stand-alone reference. It is important to complete the reading assignments from the full text by Roback and Legler in addition to studying these notes. "],["review-of-multiple-linear-regression.html", "Chapter 1 Review of Multiple Linear Regression 1.1 Exploratory Data Analysis 1.2 Simple Linear Regression Model 1.3 Multiple Linear Regression with Two Predictors 1.4 Building a Multiple Linear Regression Model", " Chapter 1 Review of Multiple Linear Regression This chapter provides an outline of Sections 1.4-1.7 of Beyond Multiple Linear Regression by Roback and Legler. Much of the text is either directly from the book, or lightly modified/summarized. Most of the code originates from the book's Github repository. knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.height = 3, fig.width = 6) # Packages required for Chapter 1 library(knitr) library(gridExtra) library(GGally) library(kableExtra) library(jtools) library(rsample) library(broom) library(tidyverse) 1.1 Exploratory Data Analysis 1.1.1 Kentucky Derby Data We use data from the Kentucky Derby, a 1.25 mile race run annually at Churchill Downs race track in Louisville, Kentucky. Our data set derbyplus.csv contains data from 1896-2017, and includes the following variables: year of the race, winning horse (winner), condition of the track, average speed (in feet per second) of the winner, number of starters (horses who raced) track condition (fast, good, slow) We would like to use least squares linear regression techniques to model the speed of the winning horse as a function of track condition, field size, and trends over time. derby.df &lt;- read.csv(&quot;https://raw.githubusercontent.com/proback/BeyondMLR/master/data/derbyplus.csv&quot;) head(derby.df) ## year winner condition speed starters ## 1 1896 Ben Brush good 51.66 8 ## 2 1897 Typhoon II slow 49.81 6 ## 3 1898 Plaudit good 51.16 4 ## 4 1899 Manuel fast 50.00 5 ## 5 1900 Lieut. Gibson fast 52.28 7 ## 6 1901 His Eminence fast 51.66 5 1.1.2 Some Data Wrangling We modify the data to create: * indicator (0-1) variables for whether the track was in fast or good condition * a factor variable (fastfactor) telling whether or not the track was fast * a variable giving years since 1896 (yearnew) derby.df &lt;- derby.df %&gt;% mutate( fast = ifelse(condition==&quot;fast&quot;,1,0), good = ifelse(condition==&quot;good&quot;,1,0), yearnew = year - 1896, fastfactor = ifelse(fast == 0, &quot;not fast&quot;, &quot;fast&quot;)) table1 &lt;- derby.df %&gt;% filter(row_number() &lt; 6 | row_number() &gt; 117) kable(table1, booktabs=T,caption=&quot;The first five and the last five observations from the Kentucky Derby case study.&quot;) %&gt;% kable_styling(latex_options = &quot;scale_down&quot;) Table 1.1: The first five and the last five observations from the Kentucky Derby case study. year winner condition speed starters fast good yearnew fastfactor 1896 Ben Brush good 51.66 8 0 1 0 not fast 1897 Typhoon II slow 49.81 6 0 0 1 not fast 1898 Plaudit good 51.16 4 0 1 2 not fast 1899 Manuel fast 50.00 5 1 0 3 fast 1900 Lieut. Gibson fast 52.28 7 1 0 4 fast 2013 Orb slow 53.71 19 0 0 117 not fast 2014 California Chrome fast 53.37 19 1 0 118 fast 2015 American Pharoah fast 53.65 18 1 0 119 fast 2016 Nyquist fast 54.41 20 1 0 120 fast 2017 Always Dreaming fast 53.40 20 1 0 121 fast 1.1.3 Univariate Graphical Summaries Distributions of winning speeds and number of starters # EDA graphs speed_hist &lt;- ggplot(data = derby.df, aes(x = speed)) + geom_histogram(binwidth = 0.5, fill = &quot;white&quot;, color = &quot;black&quot;) + xlab(&quot;Winning speed (ft/s)&quot;) + ylab(&quot;Frequency&quot;) + labs(title=&quot;(a)&quot;) starters_hist &lt;- ggplot(data = derby.df, aes(x = starters)) + geom_histogram(binwidth = 3, fill = &quot;white&quot;, color = &quot;black&quot;) + xlab(&quot;Number of starters&quot;) + ylab(&quot;Frequency&quot;) + labs(title=&quot;(b)&quot;) grid.arrange(speed_hist, starters_hist, ncol = 2) Figure 1.1: Histograms of key continuous variables. Plot (a) shows winning speeds, while plot (b) shows the number of starters. 1.1.4 Bivariate Graphical Summaries The ggpairs function creates a scatterplot matrix displaying relationships between all pairs of variables. gg &lt;- ggpairs(data = derby.df, columns = c(&quot;condition&quot;, &quot;year&quot;, &quot;starters&quot;, &quot;speed&quot;)) gg[4,1] &lt;- gg[4,1] + geom_histogram( binwidth = 0.75) gg[2,1] &lt;- gg[2,1] + geom_histogram( binwidth = 20) gg[3,1] &lt;- gg[3,1] + geom_histogram( binwidth = 3) gg Figure 1.2: Relationships between pairs of variables in the Kentucky Derby data set. We see evidence of higher speeds on fast tracks and also a tendency for recent years to have more fast conditions. We examine how winning speeds have changed over time, when the track is fast and when it is not fast. # Coded scatterplot ggplot(derby.df, aes(x = year, y = speed, colour = fastfactor)) + geom_point(aes(shape = fastfactor)) + geom_smooth(aes(linetype = fastfactor), method = lm, se = FALSE) Figure 1.3: Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions. It appears that winning speeds have increased more rapidly for tracks that are not fast. This suggests an interaction between year and track condition, since the relationship between speed and year appears to differ depending on whether or not the track was fast. 1.2 Simple Linear Regression Model 1.2.1 Model for Winning Time and Year We begin with a simple linear regression model for winning speed (\\(Y\\)), using year since 1896 as the explanatory variable. This model has the form: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\epsilon_{i} \\quad \\textrm{where} \\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2). \\end{equation}\\] We obtain estimates of \\(\\beta_0\\) and \\(\\beta_1\\), denoted (\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)) by minimizing the sum of squared residuals \\(SSR=\\displaystyle\\sum_{i=1}^{n}\\left(Y_i- (\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i})\\right)^2.\\) 1.2.2 First Model R Output We fit the model in R. model2 &lt;- lm(speed ~ yearnew, data = derby.df) coef(summary(model2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.58839264 0.162549197 317.37095 2.474501e-177 ## yearnew 0.02612601 0.002322013 11.25145 1.716806e-20 cat(&quot; R squared = &quot;, summary(model2)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model2)$sigma) ## R squared = 0.5133724 ## Residual standard error = 0.9032317 Interpretations: The expected winning speed in 1896 is 51.59 ft/s. Winning speed is expected to increase by 0.026 ft./s on average for each year since 1896. The low p-value provides evidence that average winning speed has increased over time. 51% of the total variability in winning speed is explained by the simple linear regression model with year since 1896 as the explanatory variable. We estimate that the error standard deviation \\(\\sigma\\) is 0.90. 1.2.3 Checking Model Assumptions # Residual diagnostics for Model 2 par(mar=c(1,1,1,1)) par(mfrow=c(2,2)) plot(model2) Figure 1.4: Residual plots for Model 2. par(mfrow=c(1,1)) The residual plots help tell us what trends/relationships our model is missing, or leaving unexplained. The upper left plot, Residuals vs. Fitted, can be used to check the Linearity assumption. Residuals should be patternless around Y = 0; if not, there is a pattern in the data that is currently unaccounted for. The upper right plot, Normal Q-Q, can be used to check the Normality assumption. Deviations from a straight line indicate that the distribution of residuals does not conform to a theoretical normal curve. The lower left plot, Scale-Location, can be used to check the Equal Variance assumption. Positive or negative trends across the fitted values indicate variability that is not constant. The lower right plot, Residuals vs. Leverage, can be used to check for influential points. Points with high leverage (having unusual values of the predictors) and/or high absolute residuals can have an undue influence on estimates of model parameters. There is typically no residual plot, to evaluate the Independence assumption. Evidence for lack of independence comes from knowing about the study design and methods of data collection. In this case, with a new field of horses each year, the assumption of independence is pretty reasonable. In this case, the Residuals vs. Fitted plot indicates that a quadratic fit might be better than the linear fit of Model 2; other assumptions look reasonable. 1.2.4 Quadratic Term for Year Let's add a quadratic term to the model \\[\\begin{equation*} Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2). \\end{equation*}\\] 1.2.5 Quadratic Model in R derby.df &lt;- mutate(derby.df, yearnew2 = yearnew^2) model2q &lt;- lm(speed ~ yearnew + yearnew2, data = derby.df) coef(summary(model2q)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.5874565658 2.081705e-01 243.009695 2.615174e-162 ## yearnew 0.0761728163 7.950413e-03 9.580989 1.838874e-16 ## yearnew2 -0.0004136099 6.358703e-05 -6.504628 1.920684e-09 cat(&quot; R squared = &quot;, summary(model2q)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model2q)$sigma) ## R squared = 0.6410103 ## Residual standard error = 0.7790385 # Fitted models for Model 2 and Model 2Q ggplot(derby.df, aes(x = year, y = speed)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, linetype = 1) + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), se = FALSE, linetype = 2) Figure 1.5: Linear (solid) vs. quadratic (dashed) fit. This model suggests that the rate of increase in winning speeds is slowing down over time. The low p-value on the quadratic term provides evidence that there is indeed a quadratic relationship between speed and year (as opposed to a linear one). Furthermore, the proportion of variation in winning speeds explained by the model has increased from 51.3% to 64.1%. 1.2.6 Quadratic Model Residual Plots # Residual diagnostics for Model 2 par(mar=c(1,1,1,1)) par(mfrow=c(2,2)) plot(model2q) Figure 1.6: Residual plots for Model 2Q. par(mfrow=c(1,1)) The quadratic trend in the residual vs fitted plot has disappeared, as the quadratic relationship is now explained in our model. 1.3 Multiple Linear Regression with Two Predictors 1.3.1 Model with Year and Fast Track We add an indicator variable for whether or not the track is fast in our model. Note that the text writes an indicator using the name of the 0-1 categorical variable, as opposed to the \\(\\text{I}_{\\text{Fast}}\\) notation I used in STAT 255. \\[ \\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Fast}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2). \\end{equation} \\] 1.3.2 Multiple Regression Model in R model4 &lt;- lm(speed ~ yearnew + fast, data = derby.df) coef(summary(model4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.91782155 0.154601703 329.348388 5.360308e-178 ## yearnew 0.02258276 0.001918849 11.768907 1.116763e-21 ## fast 1.22684588 0.150721259 8.139833 4.393084e-13 cat(&quot; R squared = &quot;, summary(model4)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model4)$sigma) ## R squared = 0.6874141 ## Residual standard error = 0.7269468 Interpretations: winning speeds are, on average, 1.23 ft/s faster under fast conditions after accounting for time trends (i.e. assuming year is held constant). The low p-value provides evidence that winning speeds increase over time, after accounting for track condition. winning speeds are expected to increase by 0.023 ft/s per year, after accounting for track condition. The low p-value provides evidence that winning speeds are faster when the track is in fast condition, after accounting for year. This yearly effect is also smaller than the 0.026 ft/s per year we estimated the previous model, that did not account for track condition. The single-variable model appears to have slightly overestimated the average increase in speed. This is probably because track conditions have also improved over time (due to improvements in track maintenence). The single variable model cannot distinguish between improvements in track conditions and improvements in speed of the horses. The multiple regression model can estimate these effects separately. Based on the \\(R^2\\) value, Model 4 explains 68.7% of the year-to-year variability in winning speeds, a noticeable increase over using either explanatory variable alone. 1.3.3 Confidence Intervals from MLR Model Confidence Intervals for \\(\\beta_0, \\beta_1, \\beta_2\\). Under LINE assumptions, a confidence interval for \\(\\beta_j\\) is given by \\(\\hat{\\beta_j} \\pm t_{(n-p), (1-\\alpha/2)}^* \\text{SE}(\\beta_j)\\), where \\(t_{(n-p), (1-\\alpha/2)}^*\\) represents the \\((1-\\alpha/2)\\) quantile of a t-distribution with \\(n-p\\) degrees of freedom, \\(\\alpha\\) represents the level of significance (e.g. 0.05 for a 95% CI), and \\(p\\) represents the number of parameters \\(\\beta_0, \\beta_1, \\ldots...\\) confint(model4) ## 2.5 % 97.5 % ## (Intercept) 50.61169473 51.22394836 ## yearnew 0.01878324 0.02638227 ## fast 0.92840273 1.52528902 Interpretations: We can be 95% confident that average winning speeds increase between 0.019 and 0.026 ft/s each year, after accounting for track condition. We can be 95% confident that average winning speeds under fast conditions are between 0.93 and 1.53 ft/s higher than under non-fast conditions, after accounting for the effect of year. To make a prediction for a new case, such as the winning speed in 2017, we use a prediction interval: new.data &lt;- data.frame(yearnew = 2017 - 1896, fast = 1) predict(model4, new = new.data, interval = &quot;prediction&quot;) fit lwr upr 1 54.87718 53.4143 56.34006 Based on our model, we can be 95% confident that the winning speed in 2017 under fast conditions will be between 53.4 and 56.3 ft/s. Note that Always Dreaming's actual winning speed (53.40) barely fit within this interval---the 2017 winning speed was a borderline outlier on the slow side. If we wanted to estimate the average value of Y among all cases with the given explanatory variable values, we would use interval=&quot;confidence&quot;. This doesn't really make sense in this context, since there is only one winning speed each year. 1.3.4 Slopes for Fast, non-Fast Tracks In model4, we assume that the expected rate of change in winning speed over time is the same, regardless of whether the track is fast or not. In either case, it is given by \\(\\beta_1\\). Thus, Model 4 produces a picture that looks like this: equation1 &lt;- function(x){coef(model4)[2]*x+coef(model4)[1]} equation2 &lt;- function(x){coef(model4)[2]*x+coef(model4)[1]+coef(model4)[3]} ggplot(data=derby.df, aes(x=yearnew, y=speed, color=fastfactor)) + geom_point()+ stat_function(fun=equation1,geom=&quot;line&quot;,color=scales::hue_pal()(3)[3]) + stat_function(fun=equation2,geom=&quot;line&quot;,color=scales::hue_pal()(3)[1]) Recall, however, that the data suggested that speeds have increased more rapidly for tracks that are not fast. # Coded scatterplot ggplot(derby.df, aes(x = year, y = speed, colour = fastfactor)) + geom_point(aes(shape = fastfactor)) + geom_smooth(aes(linetype = fastfactor), method = lm, se = FALSE) Figure 1.7: Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions. 1.3.5 MLR Model with Interaction We want to build a model allows winning speeds to increase at different rates for fast tracks than for those that are not fast. (i.e. a model that includes an interaction between fast and yearnew) Thus, consider Model 5: \\[ \\begin{equation*} \\begin{split} Y_{i}&amp;= \\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Fast}_{i} \\\\ &amp;{}+\\beta_{3}\\textrm{Yearnew}_{i}\\times\\textrm{Fast}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation*} \\] ### Interaction Model Estimates LLSR provides the following parameter estimates: We can do this using either of the following commands model5 &lt;- lm(speed ~ yearnew + fast + yearnew:fast, data=derby.df) model5 &lt;- lm(speed ~ yearnew*fast, data=derby.df) coef(summary(model5)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.52862926 0.205072338 246.394174 6.988530e-162 ## yearnew 0.03075099 0.003470967 8.859489 9.838736e-15 ## fast 1.83352259 0.262174513 6.993520 1.729697e-10 ## yearnew:fast -0.01149034 0.004116733 -2.791129 6.127912e-03 cat(&quot; R squared = &quot;, summary(model5)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model5)$sigma) ## R squared = 0.7067731 ## Residual standard error = 0.7070536 1.3.6 Model Equations for Fast, Non-Fast Tracks According to our model, estimated winning speeds can be found by: \\[ \\begin{equation} \\hat{Y}_{i}=50.53+0.031\\textrm{Yearnew}_{i}+1.83\\textrm{Fast}_{i}-0.011\\textrm{Yearnew}_{i}\\times\\textrm{Fast}_{i}. \\end{equation} \\] \\[ \\begin{align*} \\textrm{Fast}=0: &amp; \\\\ \\hat{Y}_{i} &amp;= 50.53+0.031\\textrm{Yearnew}_{i} \\\\ \\textrm{Fast}=1: &amp; \\\\ \\hat{Y}_{i} &amp;= (50.53+1.83)+(0.031-0.011)\\textrm{Yearnew}_{i} \\end{align*} \\] Interpretations \\(\\hat{\\beta}_{0} = 50.53\\). The expected winning speed in 1896 under non-fast conditions was 50.53 ft/s. \\(\\hat{\\beta}_{1} = 0.031\\). The expected yearly increase in winning speeds under non-fast conditions is 0.031 ft/s. \\(\\hat{\\beta}_{2} = 1.83\\). The winning speed in 1896 was expected to be 1.83 ft/s faster under fast conditions compared to non-fast conditions. \\(\\hat{\\beta}_{3} = -0.011\\). The expected yearly increase in winning speeds under fast conditions is 0.020 ft/s, compared to 0.031 ft/s under non-fast conditions, a difference of 0.011 ft/s. 1.4 Building a Multiple Linear Regression Model 1.4.1 Model Building Considerations We now add additional variables, with the goal of building a final model that provides insight into relationships between winning speed and other variables. There is no single correct model, but a good model will have the following characteristics: explanatory variables allow one to address primary research questions explanatory variables control for important covariates potential interactions have been investigated variables are centered where interpretations can be enhanced (e.g. subtract 1896 from year) unnecessary terms have been removed LINE assumptions and the presence of influential points have both been checked using residual plots the model tells a &quot;persuasive story parsimoniously&quot; Most good models should lead to similar conclusions. 1.4.2 Model Diagnostics Several tests and measures of model performance can be used when comparing different models for model building: \\(R^2\\). Measures the variability in the response variable explained by the model. One problem is that \\(R^2\\) always increases with extra predictors, even if the predictors add very little information. adjusted \\(R^2\\). Adds a penalty for model complexity to \\(R^2\\) so that any increase in performance must outweigh the cost of additional complexity. We should ideally favor any model with higher adjusted \\(R^2\\), regardless of size, but the penalty for model complexity (additional terms) is fairly ad-hoc. AIC (Akaike Information Criterion). Again attempts to balance model performance with model complexity, with smaller AIC levels being preferable, regardless of model size. The BIC (Bayesian Information Criterion) is similar to the AIC, but with a greater penalty for additional model terms. extra sum of squares F test. This is a generalization of the t-test for individual model coefficients which can be used to perform significance tests on nested models, where one model is a reduced version of the other. 1.4.3 Three Possible Models We'll consider three possible final models: Model A: \\[ \\begin{equation} \\begin{split} Y_{i}&amp;=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\beta_{3}\\textrm{Fast}_{i}\\\\ &amp;{}+\\beta_{4}\\textrm{Good}_{i}+\\beta_{5}\\textrm{Starters}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation} \\] Model B: \\[ \\begin{equation} \\begin{split} Y_{i}&amp;=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\beta_{3}\\textrm{Fast}_{i}\\\\ &amp;{}+\\beta_{4}\\textrm{Good}_{i}+\\beta_{5}\\textrm{Starters}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation} \\] Note that this is equivalent to including the original track condition variable in a model. In this case, slow track is treated as the baseline variable, since we left the indicator for slow out of the model. Model C: \\[ \\begin{equation} \\begin{split} Y_{i}&amp;=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\beta_{3}\\textrm{Fast}_{i}\\\\ &amp;{}+\\beta_{4}\\textrm{Good}_{i}+\\beta_{5}\\textrm{Starters}_{i} \\\\ &amp; + \\beta_6\\textrm{Yearnew}_{i}\\textrm{Fast}_{i}+ \\beta_7\\textrm{Yearnew}_{i}\\textrm{Good}_{i} \\\\ &amp; + \\beta_8\\textrm{Yearnew}^2_{i}\\textrm{Fast}_{i}+ \\beta_9\\textrm{Yearnew}^2_{i}\\textrm{Good}_{i} \\\\ &amp; + \\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation} \\] 1.4.4 MLR Models Fit in R We fit each model in R. model0A &lt;- lm(speed ~ yearnew + yearnew2 + starters, data = derby.df) model0B &lt;- lm(speed ~ yearnew + yearnew2 + fast + good + starters, data = derby.df) model0C &lt;- lm(speed ~ yearnew + yearnew2 + fast + good + starters + yearnew:fast + yearnew:good + yearnew2:fast + yearnew2:good, data = derby.df) 1.4.5 Model 0A Output coef(summary(model0A)) %&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.891181 0.245297 207.467371 0.000000 ## yearnew 0.083040 0.008394 9.892436 0.000000 ## yearnew2 -0.000439 0.000064 -6.905397 0.000000 ## starters -0.042392 0.018862 -2.247496 0.026467 cat(&quot; R squared = &quot;, summary(model0A)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0A)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0A)$sigma, &quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0A)) ## R squared = 0.6557468 ## Adjusted R squared = 0.6469946 ## Residual standard error = 0.7661069 ## AIC = 287.1442 1.4.6 Model 0B Output coef(summary(model0B))%&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.020315 0.194646 256.980337 0.000000 ## yearnew 0.070034 0.006130 11.423908 0.000000 ## yearnew2 -0.000370 0.000046 -8.041141 0.000000 ## fast 1.392666 0.130520 10.670102 0.000000 ## good 0.915698 0.207677 4.409248 0.000023 ## starters -0.025284 0.013602 -1.858827 0.065586 cat(&quot; R squared = &quot;, summary(model0B)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0B)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0B)$sigma,&quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0B)) ## R squared = 0.8266716 ## Adjusted R squared = 0.8192006 ## Residual standard error = 0.5482735 ## AIC = 207.4291 1.4.7 Model 0C Output coef(summary(model0C))%&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.703525 0.296614 167.569982 0.000000 ## yearnew 0.068568 0.013167 5.207395 0.000001 ## yearnew2 -0.000290 0.000102 -2.835528 0.005430 ## fast 1.697589 0.337378 5.031710 0.000002 ## good 1.704844 0.468469 3.639181 0.000415 ## starters -0.018592 0.013107 -1.418444 0.158838 ## yearnew:fast 0.004223 0.014398 0.293292 0.769841 ## yearnew:good -0.031672 0.021548 -1.469858 0.144404 ## yearnew2:fast -0.000128 0.000114 -1.124264 0.263305 ## yearnew2:good 0.000249 0.000213 1.168083 0.245254 cat(&quot; R squared = &quot;, summary(model0C)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0C)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0C)$sigma, &quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0C)) ## R squared = 0.8498007 ## Adjusted R squared = 0.8377311 ## Residual standard error = 0.5194172 ## AIC = 197.9556 1.4.8 Goodness of Fit Tests When two models are nested (that is, all the terms in the smaller model also appear in the larger model) we can compare them using a goodness of fit test. Reduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots + b_qx_{iq}\\) Full Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\) p = # parameters in Full Model q = # parameters in Reduced Model n = number of observations The hypothesis are: Null Hypothesis: Smaller model adequately eplains variability in the response variable. Alternative Hypothesis: Larger model better explains variability in the response variable than the smaller one. 1.4.9 ANOVA F-Statistic We calculate an F-statistic using the formula: \\[ F = \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\] When the null hypothesis is true, this statistic follows an F-distribution with \\(p-q\\) and \\(n-p\\) degrees of freedom. # Compare model0A and model0B anova(model0A, model0B, test = &quot;F&quot;) Analysis of Variance Table Model 1: speed ~ yearnew + yearnew2 + starters Model 2: speed ~ yearnew + yearnew2 + fast + good + starters Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 118 69.257 2 116 34.870 2 34.386 57.196 &lt; 0.00000000000000022 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is very strong evidence that track condition helps explain variability in winning speed. # Compare model0A and model0B anova(model0B, model0C, test = &quot;F&quot;) Analysis of Variance Table Model 1: speed ~ yearnew + yearnew2 + fast + good + starters Model 2: speed ~ yearnew + yearnew2 + fast + good + starters + yearnew:fast + yearnew:good + yearnew2:fast + yearnew2:good Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 116 34.870 2 112 30.217 4 4.6531 4.3117 0.002784 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence of interaction between year and track conditions. Observations: There is strong evidence that model B is better than model A. Condition of track seems to be an important factor in predicting winning speed. Models B and C both seem like reasonable fits. Asjusted R^2, AIC, and the F-test all favor model C over model B. Model C is, however, much harder to interpret. The p-values on any single interaction term were large, even though the model testing for significance of interactions collectively was small. When in doubt, it's better to go with the simpler model, unless there is clear reason to choose the more complex one. It is important to consider intuition, domain area knowledge, and interpretability when choosing a model. Do not choose a model based on statistical tests alone! 1.4.10 Final Model Residual Plots We'll go with model B. We use residual plots to check model assumptions. # Residual diagnostics for Model B par(mar=c(1,1,1,1)) par(mfrow=c(2,2)) plot(model0B) Figure 1.8: Residual plots for Model 0B. par(mfrow=c(1,1)) There do not appear to be any major model violations. Model B Coefficients Table: coef(summary(model0B))%&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.020315 0.194646 256.980337 0.000000 ## yearnew 0.070034 0.006130 11.423908 0.000000 ## yearnew2 -0.000370 0.000046 -8.041141 0.000000 ## fast 1.392666 0.130520 10.670102 0.000000 ## good 0.915698 0.207677 4.409248 0.000023 ## starters -0.025284 0.013602 -1.858827 0.065586 cat(&quot; R squared = &quot;, summary(model0B)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0B)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0B)$sigma,&quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0B)) ## R squared = 0.8266716 ## Adjusted R squared = 0.8192006 ## Residual standard error = 0.5482735 ## AIC = 207.4291 1.4.11 Overall Conclusions Conclusions: * The rate of increase in winning speeds is slowing over time (negative quadratic term) * The better the condition of the track, the faster the horses tend to run * larger field, with more starters, is associated with slower winning times Notice this last conclusion appears contradictory to our exploratory data analysis, which showed a positive relationship between starters and speed. gg &lt;- ggpairs(data = derby.df, columns = c(&quot;condition&quot;, &quot;year&quot;, &quot;starters&quot;, &quot;speed&quot;)) gg[4,1] &lt;- gg[4,1] + geom_histogram( binwidth = 0.75) gg[2,1] &lt;- gg[2,1] + geom_histogram( binwidth = 20) gg[3,1] &lt;- gg[3,1] + geom_histogram( binwidth = 3) gg Figure 1.9: Relationships between pairs of variables in the Kentucky Derby data set. This happens because over time, the number of starters in the race has increased, as have winning speeds. So, it appears that having more starters is associated with faster winning speeds, but year is acting as a confounding variable. The multiple regression model is able to separate the effect of year from that of number of starters. The model tells us that assuming year is held constant, having more starters is actually associated with a slower winning speed. A situation like this, where adding a variable (such as year) to a model results in an apparent trend disappearing or reversing itself, is called Simpson's Paradox. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
