[["index.html", "Stat 455: Advanced Statistical Modeling Notes Preface", " Stat 455: Advanced Statistical Modeling Notes 2022-01-07 Preface These notes are written to accompany the text Beyond Multiple Linear Regression by Roback and Legler. Much of the text is either directly from the book, or lightly modified/summarized. Most of the code originates from the book's Github repository. These notes will guide our lectures and class discussion, but they are not sufficient as a stand-alone reference. It is important to complete the reading assignments from the full text by Roback and Legler in addition to studying these notes. "],["review-of-multiple-linear-regression.html", "Chapter 1 Review of Multiple Linear Regression 1.1 Exploratory Data Analysis 1.2 Simple Linear Regression Model 1.3 Multiple Linear Regression with Two Predictors 1.4 Building a Multiple Linear Regression Model", " Chapter 1 Review of Multiple Linear Regression This chapter provides an outline of Sections 1.4-1.7 of Beyond Multiple Linear Regression by Roback and Legler. Much of the text is either directly from the book, or lightly modified/summarized. Most of the code originates from the book's Github repository. knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.height = 3, fig.width = 6) # Packages required for Chapter 1 library(knitr) library(gridExtra) library(GGally) library(kableExtra) library(jtools) library(rsample) library(broom) library(tidyverse) 1.1 Exploratory Data Analysis 1.1.1 Kentucky Derby Data We use data from the Kentucky Derby, a 1.25 mile race run annually at Churchill Downs race track in Louisville, Kentucky. Our data set derbyplus.csv contains data from 1896-2017, and includes the following variables: year of the race, winning horse (winner), condition of the track (fast, good, slow) , average speed (in feet per second) of the winner, number of starters (horses who raced) We would like to use least squares linear regression techniques to model the speed of the winning horse as a function of track condition, field size, and trends over time. derby.df &lt;- read.csv(&quot;https://raw.githubusercontent.com/proback/BeyondMLR/master/data/derbyplus.csv&quot;) head(derby.df) ## year winner condition speed starters ## 1 1896 Ben Brush good 51.66 8 ## 2 1897 Typhoon II slow 49.81 6 ## 3 1898 Plaudit good 51.16 4 ## 4 1899 Manuel fast 50.00 5 ## 5 1900 Lieut. Gibson fast 52.28 7 ## 6 1901 His Eminence fast 51.66 5 1.1.2 Some Data Wrangling We modify the data to create: * indicator (0-1) variables for whether the track was in fast or good condition * a factor variable (fastfactor) telling whether or not the track was fast * a variable giving years since 1896 (yearnew) derby.df &lt;- derby.df %&gt;% mutate( fast = ifelse(condition==&quot;fast&quot;,1,0), good = ifelse(condition==&quot;good&quot;,1,0), yearnew = year - 1896, fastfactor = ifelse(fast == 0, &quot;not fast&quot;, &quot;fast&quot;)) table1 &lt;- derby.df %&gt;% filter(row_number() &lt; 6 | row_number() &gt; 117) kable(table1, booktabs=T,caption=&quot;The first five and the last five observations from the Kentucky Derby case study.&quot;) %&gt;% kable_styling(latex_options = &quot;scale_down&quot;) Table 1.1: The first five and the last five observations from the Kentucky Derby case study. year winner condition speed starters fast good yearnew fastfactor 1896 Ben Brush good 51.66 8 0 1 0 not fast 1897 Typhoon II slow 49.81 6 0 0 1 not fast 1898 Plaudit good 51.16 4 0 1 2 not fast 1899 Manuel fast 50.00 5 1 0 3 fast 1900 Lieut. Gibson fast 52.28 7 1 0 4 fast 2013 Orb slow 53.71 19 0 0 117 not fast 2014 California Chrome fast 53.37 19 1 0 118 fast 2015 American Pharoah fast 53.65 18 1 0 119 fast 2016 Nyquist fast 54.41 20 1 0 120 fast 2017 Always Dreaming fast 53.40 20 1 0 121 fast 1.1.3 Univariate Graphical Summaries Distributions of winning speeds and number of starters # EDA graphs speed_hist &lt;- ggplot(data = derby.df, aes(x = speed)) + geom_histogram(binwidth = 0.5, fill = &quot;white&quot;, color = &quot;black&quot;) + xlab(&quot;Winning speed (ft/s)&quot;) + ylab(&quot;Frequency&quot;) + labs(title=&quot;(a)&quot;) starters_hist &lt;- ggplot(data = derby.df, aes(x = starters)) + geom_histogram(binwidth = 3, fill = &quot;white&quot;, color = &quot;black&quot;) + xlab(&quot;Number of starters&quot;) + ylab(&quot;Frequency&quot;) + labs(title=&quot;(b)&quot;) grid.arrange(speed_hist, starters_hist, ncol = 2) Figure 1.1: Histograms of key continuous variables. Plot (a) shows winning speeds, while plot (b) shows the number of starters. 1.1.4 Bivariate Graphical Summaries The ggpairs function creates a scatterplot matrix displaying relationships between all pairs of variables. gg &lt;- ggpairs(data = derby.df, columns = c(&quot;condition&quot;, &quot;year&quot;, &quot;starters&quot;, &quot;speed&quot;)) gg Figure 1.2: Relationships between pairs of variables in the Kentucky Derby data set. We see evidence of higher speeds on fast tracks and also a tendency for recent years to have more fast conditions. We examine how winning speeds have changed over time, when the track is fast and when it is not fast. # Coded scatterplot ggplot(derby.df, aes(x = year, y = speed, colour = fastfactor)) + geom_point(aes(shape = fastfactor)) + geom_smooth(aes(linetype = fastfactor), method = lm, se = FALSE) Figure 1.3: Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions. It appears that winning speeds have increased more rapidly for tracks that are not fast. This suggests an interaction between year and track condition, since the relationship between speed and year appears to differ depending on whether or not the track was fast. 1.2 Simple Linear Regression Model 1.2.1 Model for Winning Time and Year We begin with a simple linear regression model for winning speed (\\(Y\\)), using year since 1896 as the explanatory variable. This model has the form: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\epsilon_{i} \\quad \\textrm{where} \\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2). \\end{equation}\\] We obtain estimates of \\(\\beta_0\\) and \\(\\beta_1\\), denoted (\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)) by minimizing the sum of squared residuals \\(SSR=\\displaystyle\\sum_{i=1}^{n}\\left(Y_i- (\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i})\\right)^2.\\) 1.2.2 First Model R Output We fit the model in R. model2 &lt;- lm(speed ~ yearnew, data = derby.df) coef(summary(model2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.58839264 0.162549197 317.37095 2.474501e-177 ## yearnew 0.02612601 0.002322013 11.25145 1.716806e-20 cat(&quot; R squared = &quot;, summary(model2)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model2)$sigma) ## R squared = 0.5133724 ## Residual standard error = 0.9032317 Interpretations: The expected winning speed in 1896 is 51.59 ft/s. Winning speed is expected to increase by 0.026 ft./s on average for each year since 1896. The low p-value provides evidence that average winning speed has increased over time. 51% of the total variability in winning speed is explained by the simple linear regression model with year since 1896 as the explanatory variable. We estimate that the error standard deviation \\(\\sigma\\) is 0.90. 1.2.3 Checking Model Assumptions # Residual diagnostics for Model 2 par(mar=c(4,4,4,4)) par(mfrow=c(2,2)) plot(model2) Figure 1.4: Residual plots for Model 2. par(mfrow=c(1,1)) The residual plots help tell us what trends/relationships our model is missing, or leaving unexplained. The upper left plot, Residuals vs. Fitted, can be used to check the Linearity assumption. Residuals should be patternless around Y = 0; if not, there is a pattern in the data that is currently unaccounted for. The upper right plot, Normal Q-Q, can be used to check the Normality assumption. Deviations from a straight line indicate that the distribution of residuals does not conform to a theoretical normal curve. The lower left plot, Scale-Location, can be used to check the Equal Variance assumption. Positive or negative trends across the fitted values indicate variability that is not constant. The lower right plot, Residuals vs. Leverage, can be used to check for influential points. Points with high leverage (having unusual values of the predictors) and/or high absolute residuals can have an undue influence on estimates of model parameters. There is typically no residual plot, to evaluate the Independence assumption. Evidence for lack of independence comes from knowing about the study design and methods of data collection. In this case, with a new field of horses each year, the assumption of independence is pretty reasonable. In this case, the Residuals vs. Fitted plot indicates that a quadratic fit might be better than the linear fit of Model 2; other assumptions look reasonable. 1.2.4 Quadratic Term for Year Let's add a quadratic term to the model \\[\\begin{equation*} Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2). \\end{equation*}\\] 1.2.5 Quadratic Model in R derby.df &lt;- mutate(derby.df, yearnew2 = yearnew^2) model2q &lt;- lm(speed ~ yearnew + yearnew2, data = derby.df) coef(summary(model2q)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.5874565658 2.081705e-01 243.009695 2.615174e-162 ## yearnew 0.0761728163 7.950413e-03 9.580989 1.838874e-16 ## yearnew2 -0.0004136099 6.358703e-05 -6.504628 1.920684e-09 cat(&quot; R squared = &quot;, summary(model2q)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model2q)$sigma) ## R squared = 0.6410103 ## Residual standard error = 0.7790385 # Fitted models for Model 2 and Model 2Q ggplot(derby.df, aes(x = year, y = speed)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, linetype = 1) + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), se = FALSE, linetype = 2) Figure 1.5: Linear (solid) vs. quadratic (dashed) fit. This model suggests that the rate of increase in winning speeds is slowing down over time. The low p-value on the quadratic term provides evidence that there is indeed a quadratic relationship between speed and year (as opposed to a linear one). Furthermore, the proportion of variation in winning speeds explained by the model has increased from 51.3% to 64.1%. 1.2.6 Quadratic Model Residual Plots # Residual diagnostics for Model 2 par(mar=c(4,4,4,4)) par(mfrow=c(2,2)) plot(model2q) Figure 1.6: Residual plots for Model 2Q. par(mfrow=c(1,1)) The quadratic trend in the residual vs fitted plot has disappeared, as the quadratic relationship is now explained in our model. 1.3 Multiple Linear Regression with Two Predictors 1.3.1 Model with Year and Fast Track We add an indicator variable for whether or not the track is fast in our model. Note that the text writes an indicator using the name of the 0-1 categorical variable, as opposed to the \\(\\text{I}_{\\text{Fast}}\\) notation I used in STAT 255. \\[ \\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Fast}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2). \\end{equation} \\] 1.3.2 Multiple Regression Model in R model4 &lt;- lm(speed ~ yearnew + fast, data = derby.df) coef(summary(model4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.91782155 0.154601703 329.348388 5.360308e-178 ## yearnew 0.02258276 0.001918849 11.768907 1.116763e-21 ## fast 1.22684588 0.150721259 8.139833 4.393084e-13 cat(&quot; R squared = &quot;, summary(model4)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model4)$sigma) ## R squared = 0.6874141 ## Residual standard error = 0.7269468 Interpretations: winning speeds are, on average, 1.23 ft/s faster under fast conditions after accounting for time trends (i.e. assuming year is held constant). The low p-value provides evidence that winning speeds increase over time, after accounting for track condition. winning speeds are expected to increase by 0.023 ft/s per year, after accounting for track condition. The low p-value provides evidence that winning speeds are faster when the track is in fast condition, after accounting for year. This yearly effect is also smaller than the 0.026 ft/s per year we estimated the previous model, that did not account for track condition. The single-variable model appears to have slightly overestimated the average increase in speed. This is probably because track conditions have also improved over time (due to improvements in track maintenence). The single variable model cannot distinguish between improvements in track conditions and improvements in speed of the horses. The multiple regression model can estimate these effects separately. Based on the \\(R^2\\) value, Model 4 explains 68.7% of the year-to-year variability in winning speeds, a noticeable increase over using either explanatory variable alone. 1.3.3 Confidence Intervals from MLR Model Confidence Intervals for \\(\\beta_0, \\beta_1, \\beta_2\\). Under LINE assumptions, a confidence interval for \\(\\beta_j\\) is given by \\(\\hat{\\beta_j} \\pm t_{(n-p), (1-\\alpha/2)}^* \\text{SE}(\\beta_j)\\), where \\(t_{(n-p), (1-\\alpha/2)}^*\\) represents the \\((1-\\alpha/2)\\) quantile of a t-distribution with \\(n-p\\) degrees of freedom, \\(\\alpha\\) represents the level of significance (e.g. 0.05 for a 95% CI), and \\(p\\) represents the number of parameters \\(\\beta_0, \\beta_1, \\ldots...\\) confint(model4) ## 2.5 % 97.5 % ## (Intercept) 50.61169473 51.22394836 ## yearnew 0.01878324 0.02638227 ## fast 0.92840273 1.52528902 Interpretations: We can be 95% confident that average winning speeds increase between 0.019 and 0.026 ft/s each year, after accounting for track condition. We can be 95% confident that average winning speeds under fast conditions are between 0.93 and 1.53 ft/s higher than under non-fast conditions, after accounting for the effect of year. To make a prediction for a new case, such as the winning speed in 2017, we use a prediction interval: new.data &lt;- data.frame(yearnew = 2017 - 1896, fast = 1) predict(model4, new = new.data, interval = &quot;prediction&quot;) fit lwr upr 1 54.87718 53.4143 56.34006 Based on our model, we can be 95% confident that the winning speed in 2017 under fast conditions will be between 53.4 and 56.3 ft/s. Note that Always Dreaming's actual winning speed (53.40) barely fit within this interval---the 2017 winning speed was a borderline outlier on the slow side. If we wanted to estimate the average value of Y among all cases with the given explanatory variable values, we would use interval=&quot;confidence&quot;. This doesn't really make sense in this context, since there is only one winning speed each year. 1.3.4 Slopes for Fast, non-Fast Tracks In model4, we assume that the expected rate of change in winning speed over time is the same, regardless of whether the track is fast or not. In either case, it is given by \\(\\beta_1\\). Thus, Model 4 produces a picture that looks like this: equation1 &lt;- function(x){coef(model4)[2]*x+coef(model4)[1]} equation2 &lt;- function(x){coef(model4)[2]*x+coef(model4)[1]+coef(model4)[3]} ggplot(data=derby.df, aes(x=yearnew, y=speed, color=fastfactor)) + geom_point()+ stat_function(fun=equation1,geom=&quot;line&quot;,color=scales::hue_pal()(3)[3]) + stat_function(fun=equation2,geom=&quot;line&quot;,color=scales::hue_pal()(3)[1]) Recall, however, that the data suggested that speeds have increased more rapidly for tracks that are not fast. # Coded scatterplot ggplot(derby.df, aes(x = year, y = speed, colour = fastfactor)) + geom_point(aes(shape = fastfactor)) + geom_smooth(aes(linetype = fastfactor), method = lm, se = FALSE) Figure 1.7: Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions. 1.3.5 MLR Model with Interaction We want to build a model allows winning speeds to increase at different rates for fast tracks than for those that are not fast. (i.e. a model that includes an interaction between fast and yearnew) Thus, consider Model 5: \\[ \\begin{equation*} \\begin{split} Y_{i}&amp;= \\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Fast}_{i} \\\\ &amp;{}+\\beta_{3}\\textrm{Yearnew}_{i}\\times\\textrm{Fast}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation*} \\] ### Interaction Model Estimates LLSR provides the following parameter estimates: We can do this using either of the following commands model5 &lt;- lm(speed ~ yearnew + fast + yearnew:fast, data=derby.df) model5 &lt;- lm(speed ~ yearnew*fast, data=derby.df) coef(summary(model5)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.52862926 0.205072338 246.394174 6.988530e-162 ## yearnew 0.03075099 0.003470967 8.859489 9.838736e-15 ## fast 1.83352259 0.262174513 6.993520 1.729697e-10 ## yearnew:fast -0.01149034 0.004116733 -2.791129 6.127912e-03 cat(&quot; R squared = &quot;, summary(model5)$r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model5)$sigma) ## R squared = 0.7067731 ## Residual standard error = 0.7070536 1.3.6 Model Equations for Fast, Non-Fast Tracks According to our model, estimated winning speeds can be found by: \\[ \\begin{equation} \\hat{Y}_{i}=50.53+0.031\\textrm{Yearnew}_{i}+1.83\\textrm{Fast}_{i}-0.011\\textrm{Yearnew}_{i}\\times\\textrm{Fast}_{i}. \\end{equation} \\] \\[ \\begin{align*} \\textrm{Fast}=0: &amp; \\\\ \\hat{Y}_{i} &amp;= 50.53+0.031\\textrm{Yearnew}_{i} \\\\ \\textrm{Fast}=1: &amp; \\\\ \\hat{Y}_{i} &amp;= (50.53+1.83)+(0.031-0.011)\\textrm{Yearnew}_{i} \\end{align*} \\] Interpretations \\(\\hat{\\beta}_{0} = 50.53\\). The expected winning speed in 1896 under non-fast conditions was 50.53 ft/s. \\(\\hat{\\beta}_{1} = 0.031\\). The expected yearly increase in winning speeds under non-fast conditions is 0.031 ft/s. \\(\\hat{\\beta}_{2} = 1.83\\). The winning speed in 1896 was expected to be 1.83 ft/s faster under fast conditions compared to non-fast conditions. \\(\\hat{\\beta}_{3} = -0.011\\). The expected yearly increase in winning speeds under fast conditions is 0.020 ft/s, compared to 0.031 ft/s under non-fast conditions, a difference of 0.011 ft/s. 1.4 Building a Multiple Linear Regression Model 1.4.1 Model Building Considerations We now add additional variables, with the goal of building a final model that provides insight into relationships between winning speed and other variables. There is no single correct model, but a good model will have the following characteristics: explanatory variables allow one to address primary research questions explanatory variables control for important covariates potential interactions have been investigated variables are centered where interpretations can be enhanced (e.g. subtract 1896 from year) unnecessary terms have been removed LINE assumptions and the presence of influential points have both been checked using residual plots the model tells a &quot;persuasive story parsimoniously&quot; Most good models should lead to similar conclusions. 1.4.2 Model Diagnostics Several tests and measures of model performance can be used when comparing different models for model building: \\(R^2\\). Measures the variability in the response variable explained by the model. One problem is that \\(R^2\\) always increases with extra predictors, even if the predictors add very little information. adjusted \\(R^2\\). Adds a penalty for model complexity to \\(R^2\\) so that any increase in performance must outweigh the cost of additional complexity. We should ideally favor any model with higher adjusted \\(R^2\\), regardless of size, but the penalty for model complexity (additional terms) is fairly ad-hoc. AIC (Akaike Information Criterion). Again attempts to balance model performance with model complexity, with smaller AIC levels being preferable, regardless of model size. The BIC (Bayesian Information Criterion) is similar to the AIC, but with a greater penalty for additional model terms. extra sum of squares F test. This is a generalization of the t-test for individual model coefficients which can be used to perform significance tests on nested models, where one model is a reduced version of the other. 1.4.3 Three Possible Models We'll consider three possible final models: Model A: \\[ \\begin{equation} \\begin{split} Y_{i}&amp;=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation} \\] Model B: \\[ \\begin{equation} \\begin{split} Y_{i}&amp;=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\beta_{3}\\textrm{Fast}_{i}\\\\ &amp;{}+\\beta_{4}\\textrm{Good}_{i}+\\beta_{5}\\textrm{Starters}_{i}+\\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation} \\] Note that this is equivalent to including the original track condition variable in a model. In this case, slow track is treated as the baseline variable, since we left the indicator for slow out of the model. Model C: \\[ \\begin{equation} \\begin{split} Y_{i}&amp;=\\beta_{0}+\\beta_{1}\\textrm{Yearnew}_{i}+\\beta_{2}\\textrm{Yearnew}^2_{i}+\\beta_{3}\\textrm{Fast}_{i}\\\\ &amp;{}+\\beta_{4}\\textrm{Good}_{i}+\\beta_{5}\\textrm{Starters}_{i} \\\\ &amp; + \\beta_6\\textrm{Yearnew}_{i}\\textrm{Fast}_{i}+ \\beta_7\\textrm{Yearnew}_{i}\\textrm{Good}_{i} \\\\ &amp; + \\beta_8\\textrm{Yearnew}^2_{i}\\textrm{Fast}_{i}+ \\beta_9\\textrm{Yearnew}^2_{i}\\textrm{Good}_{i} \\\\ &amp; + \\epsilon_{i}\\quad \\textrm{where}\\quad \\epsilon_{i}\\sim \\textrm{N}(0,\\sigma^2) \\end{split} \\end{equation} \\] 1.4.4 MLR Models Fit in R We fit each model in R. model0A &lt;- lm(speed ~ yearnew + yearnew2 , data = derby.df) model0B &lt;- lm(speed ~ yearnew + yearnew2 + fast + good + starters, data = derby.df) model0C &lt;- lm(speed ~ yearnew + yearnew2 + fast + good + starters + yearnew:fast + yearnew:good + yearnew2:fast + yearnew2:good, data = derby.df) 1.4.5 Model 0A Output coef(summary(model0A)) %&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.587457 0.208171 243.009695 0 ## yearnew 0.076173 0.007950 9.580989 0 ## yearnew2 -0.000414 0.000064 -6.504628 0 cat(&quot; R squared = &quot;, summary(model0A)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0A)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0A)$sigma, &quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0A)) ## R squared = 0.6410103 ## Adjusted R squared = 0.6349769 ## Residual standard error = 0.7790385 ## AIC = 290.258 1.4.6 Model 0B Output coef(summary(model0B))%&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.020315 0.194646 256.980337 0.000000 ## yearnew 0.070034 0.006130 11.423908 0.000000 ## yearnew2 -0.000370 0.000046 -8.041141 0.000000 ## fast 1.392666 0.130520 10.670102 0.000000 ## good 0.915698 0.207677 4.409248 0.000023 ## starters -0.025284 0.013602 -1.858827 0.065586 cat(&quot; R squared = &quot;, summary(model0B)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0B)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0B)$sigma,&quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0B)) ## R squared = 0.8266716 ## Adjusted R squared = 0.8192006 ## Residual standard error = 0.5482735 ## AIC = 207.4291 1.4.7 Model 0C Output coef(summary(model0C))%&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.703525 0.296614 167.569982 0.000000 ## yearnew 0.068568 0.013167 5.207395 0.000001 ## yearnew2 -0.000290 0.000102 -2.835528 0.005430 ## fast 1.697589 0.337378 5.031710 0.000002 ## good 1.704844 0.468469 3.639181 0.000415 ## starters -0.018592 0.013107 -1.418444 0.158838 ## yearnew:fast 0.004223 0.014398 0.293292 0.769841 ## yearnew:good -0.031672 0.021548 -1.469858 0.144404 ## yearnew2:fast -0.000128 0.000114 -1.124264 0.263305 ## yearnew2:good 0.000249 0.000213 1.168083 0.245254 cat(&quot; R squared = &quot;, summary(model0C)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0C)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0C)$sigma, &quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0C)) ## R squared = 0.8498007 ## Adjusted R squared = 0.8377311 ## Residual standard error = 0.5194172 ## AIC = 197.9556 1.4.8 Goodness of Fit Tests When two models are nested (that is, all the terms in the smaller model also appear in the larger model) we can compare them using a goodness of fit test. Reduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots + b_qx_{iq}\\) Full Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\) p = # parameters in Full Model q = # parameters in Reduced Model n = number of observations The hypothesis are: Null Hypothesis: Smaller model adequately eplains variability in the response variable. Alternative Hypothesis: Larger model better explains variability in the response variable than the smaller one. 1.4.9 ANOVA F-Statistic We calculate an F-statistic using the formula: \\[ F = \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\] When the null hypothesis is true, this statistic follows an F-distribution with \\(p-q\\) and \\(n-p\\) degrees of freedom. # Compare model0A and model0B anova(model0A, model0B, test = &quot;F&quot;) Analysis of Variance Table Model 1: speed ~ yearnew + yearnew2 Model 2: speed ~ yearnew + yearnew2 + fast + good + starters Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 119 72.221 2 116 34.870 3 37.351 41.418 &lt; 0.00000000000000022 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is very strong evidence that track condition and number of starters help explain variability in winning speed. # Compare model0A and model0B anova(model0B, model0C, test = &quot;F&quot;) Analysis of Variance Table Model 1: speed ~ yearnew + yearnew2 + fast + good + starters Model 2: speed ~ yearnew + yearnew2 + fast + good + starters + yearnew:fast + yearnew:good + yearnew2:fast + yearnew2:good Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 116 34.870 2 112 30.217 4 4.6531 4.3117 0.002784 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence of interaction between year and track conditions. Observations: There is strong evidence that model B is better than model A. Accounting for condition of track and number of starters helps explain variability in winning speeds. Models B and C both seem like reasonable fits. Adjusted R^2, AIC, and the F-test all favor model C over model B. Model C is, however, much harder to interpret. The p-values on any single interaction term were large, even though the model testing for significance of interactions collectively was small. When in doubt, it's better to go with the simpler model, unless there is clear reason to choose the more complex one. It is important to consider intuition, domain area knowledge, and interpretability when choosing a model. Do not choose a model based on statistical tests alone! 1.4.10 Final Model Residual Plots We'll go with model B. We use residual plots to check model assumptions. # Residual diagnostics for Model B par(mar=c(4,4,4,4)) par(mfrow=c(2,2)) plot(model0B) Figure 1.8: Residual plots for Model 0B. par(mfrow=c(1,1)) There do not appear to be any major model violations. Model B Coefficients Table: coef(summary(model0B))%&gt;% round(6) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.020315 0.194646 256.980337 0.000000 ## yearnew 0.070034 0.006130 11.423908 0.000000 ## yearnew2 -0.000370 0.000046 -8.041141 0.000000 ## fast 1.392666 0.130520 10.670102 0.000000 ## good 0.915698 0.207677 4.409248 0.000023 ## starters -0.025284 0.013602 -1.858827 0.065586 cat(&quot; R squared = &quot;, summary(model0B)$r.squared, &quot;\\n&quot;, &quot; Adjusted R squared = &quot;, summary(model0B)$adj.r.squared, &quot;\\n&quot;, &quot;Residual standard error = &quot;, summary(model0B)$sigma,&quot;\\n&quot;, &quot;AIC = &quot;, AIC(model0B)) ## R squared = 0.8266716 ## Adjusted R squared = 0.8192006 ## Residual standard error = 0.5482735 ## AIC = 207.4291 1.4.11 Overall Conclusions Conclusions: * The rate of increase in winning speeds is slowing over time (negative quadratic term) * The better the condition of the track, the faster the horses tend to run * larger field, with more starters, is associated with slower winning times Notice this last conclusion appears contradictory to our exploratory data analysis, which showed a positive relationship between starters and speed. gg &lt;- ggpairs(data = derby.df, columns = c(&quot;condition&quot;, &quot;year&quot;, &quot;starters&quot;, &quot;speed&quot;)) gg Figure 1.9: Relationships between pairs of variables in the Kentucky Derby data set. This happens because over time, the number of starters in the race has increased, as have winning speeds. So, it appears that having more starters is associated with faster winning speeds, but year is acting as a confounding variable. The multiple regression model is able to separate the effect of year from that of number of starters. The model tells us that assuming year is held constant, having more starters is actually associated with a slower winning speed. A situation like this, where adding a variable (such as year) to a model results in an apparent trend disappearing or reversing itself, is called Simpson's Paradox. "],["introduction-to-correlated-data.html", "Chapter 2 Introduction to Correlated Data 2.1 Introduction to Correlated Data 2.2 Linear Mixed Effects Models 2.3 A Second Mice Experiment 2.4 A Multilevel Experiment", " Chapter 2 Introduction to Correlated Data library(tidyverse) library(lme4) library(lmerTest) library(knitr) 2.1 Introduction to Correlated Data 2.1.1 Weight Gain in Mice: Experiment Design #1 Consider an experiment designed to assess the impact of three different diets on weight gain in mice. We observe six different litters of mice, with six mice in each litter. Within each litter, two mice are randomly assigned to each of the three diets. Researchers recorded the mean weight gain in each mouse over a four-week period. Experimental Design 1: Figure 2.1: Mouse Experiment Version 1 2.1.2 Mice Experiment 1 Data The first 10 rows of the dataset look like this: head(mice,10) ## litter diet weight_gain ## 1 1 A -0.03526041 ## 2 1 B -0.06841666 ## 3 1 C -0.06351305 ## 4 1 A -0.05171452 ## 5 1 B -0.06917391 ## 6 1 C -0.05951775 ## 7 2 A 0.07584083 ## 8 2 B 0.04913527 ## 9 2 C 0.07958384 ## 10 2 A 0.07278071 2.1.3 A Naive Graphical Analysis Let's temporarily ignore the fact that some mice came from the same litter, and treat all observations as independent. The plot below shows the weight gain (or loss) for each of the mice, by diet. The red dots and connecting lines show the mean weight gain for each diet. The ensuing table shows the mean and standard deviation in weight gain for each of the three diets. ggplot(data=mice, aes(x=factor(diet), y=weight_gain)) + geom_point() + stat_summary(fun=&quot;mean&quot;, geom=&quot;line&quot;, aes(group=factor(1))) + stat_summary(fun=&quot;mean&quot;, geom=&quot;point&quot;, color=&quot;red&quot;, size=2) Question: Based on this graph, do you think there is evidence of diets have an effect on weight gain in mice? Why or why not? 2.1.4 A More Informed Graphical Analysis So far, we've ignored the fact that some of the mice came from the same litter. Now, let's take litter into account. The figure below colors the mice by litter. The lines represent the average weight gain (or loss) in each litter. ggplot(data=mice, aes(x=factor(diet), y=weight_gain, color=factor(litter))) + geom_point() + stat_summary(fun=&quot;mean&quot;, geom=&quot;line&quot;, aes(group=factor(litter))) It may be helpful to examine each litter individually. ggplot(data=mice, aes(x=factor(diet), y=weight_gain, color=factor(litter))) + geom_point() + facet_grid(.~litter, scales = &quot;free&quot;) + stat_summary(fun=&quot;mean&quot;, geom=&quot;line&quot;, aes(group=factor(litter))) Question: Based on the information about litters, do your thoughts about whether diets have an effect on weight gain in mice change? Does there appear to be stronger evidence of differences between groups? Weaker? Same? 2.1.5 Table of Mean Weight By Diet mouse_groups &lt;- mice %&gt;% group_by(diet)%&gt;% summarize(Mean_Weight=mean(weight_gain), SD_Weight = sd(weight_gain), N=n()) mouse_groups ## # A tibble: 3 x 4 ## diet Mean_Weight SD_Weight N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 0.0580 0.0567 12 ## 2 B 0.0340 0.0583 12 ## 3 C 0.0471 0.0612 12 2.1.6 Table of Mean Weight By Diet and Litter mouse_groups &lt;- mice %&gt;% group_by(diet, litter)%&gt;% summarize(Mean_Weight=mean(weight_gain), SD_Weight = sd(weight_gain), N=n()) mouse_groups ## # A tibble: 18 x 5 ## # Groups: diet [3] ## diet litter Mean_Weight SD_Weight N ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 1 -0.0435 0.0116 2 ## 2 A 2 0.0743 0.00216 2 ## 3 A 3 0.0192 0.00282 2 ## 4 A 4 0.104 0.00487 2 ## 5 A 5 0.110 0.00145 2 ## 6 A 6 0.0833 0.00721 2 ## 7 B 1 -0.0688 0.000535 2 ## 8 B 2 0.0554 0.00881 2 ## 9 B 3 -0.0108 0.00781 2 ## 10 B 4 0.0731 0.00482 2 ## 11 B 5 0.0873 0.00890 2 ## 12 B 6 0.0682 0.00430 2 ## 13 C 1 -0.0615 0.00283 2 ## 14 C 2 0.0746 0.00711 2 ## 15 C 3 0.000839 0.00489 2 ## 16 C 4 0.0918 0.00638 2 ## 17 C 5 0.0979 0.0214 2 ## 18 C 6 0.0792 0.00310 2 Since each mouse in a litter received a different treatment, we can use the standard deviations between mice in the same litter to assess the amount of unexplained variability after accounting for litter and diet. Notice standard deviations are much smaller when we account for litter. Most of the unexplained variability in the first table is explained when we account for litter. When we fail to account for litter, variability thqt can be explained by differences between litters becomes conflated with unexplained variability. This causes us to overestimate unexplained variability and makes differences between groups look less meaningful than they really are. 2.1.7 Assessing Evidence of Differences Conceptually, we can assess whether there is evidence of differences between the diets by considering differences in mean weights, relative to the amount of unexplained variability between mice in the same litter and on the same diet. Recall that when testing for a difference between two groups, a t-statistic is calculated using the formula \\[ t= \\frac{\\bar{x}_1-\\bar{x}_2}{\\textrm{SE}(\\bar{x}_1-\\bar{x}_2)}=\\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}} \\] The numerator measures the size of the differences between the groups, and the denominator measures the amount of unexplained variability between individuals in the same group. 2.1.8 An Improper Statistical Analysis We've seen graphically, and in table form, how accounting for litter impacts our ability to discern differences between diets. Now, let's look at how this happens in a statistical model. If we ignore the fact that some mice are from the same litter, and wrongly treat them as independent, we might use an ordinary linear least squares regression model. M_LLSR &lt;- lm(data=mice, weight_gain~factor(diet)) summary(M_LLSR)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05798494 0.01696450 3.4180166 0.001693572 ## factor(diet)B -0.02394185 0.02399142 -0.9979334 0.325573272 ## factor(diet)C -0.01085590 0.02399142 -0.4524907 0.653876252 The p-values on line 2 is large, indicating that there is not evidence of differences in weight gain between mice on diet 2, and the baseline diet (diet 1). The same is true for a comparison of diets 3 and 1, as seen on the third line. 2.1.9 A More Appropriate Statistical Analysis The following command fits a linear mixed effects model (or multilevel model) that accounts for correlation between mice in the same litter. M_LME &lt;- lmer(data=mice, weight_gain~factor(diet) + (1 | factor(litter))) summary(M_LME)$coeff ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.05798494 0.025057548 5.046907 2.314071 0.068074358859032 ## factor(diet)B -0.02394185 0.002962717 28.000000 -8.081043 0.000000008464652 ## factor(diet)C -0.01085590 0.002962717 28.000000 -3.664169 0.001025965687040 Estimates are unchanged, but standard errors decrease by a factor of almost 10. t-statistics are now high, and p-values low, indicating differences between the diets Just as we saw graphically, and in tables, accounting for differences between litters allows us to accurately quantify unexplained variability, and assess whether there is evidence of differences in weight gain between the diets. 2.2 Linear Mixed Effects Models 2.2.1 LLSR Model for Mice Experiment Let \\(Y_{ij}\\) denote the weight gain of mouse \\(j\\) in litter \\(i\\). \\(j=1,2,\\ldots, 6\\), \\(j=1,2,\\ldots, 6\\). In an ordinary linear least squares regression model, we assume that: each diet has an expected (or average) weight gain individual mice vary from their expected weights randomly, according to normal distributions with constant variance \\(\\sigma^2\\) no two mice are any more or less alike than any others, except for diet (which is not true in this context) A model would have the form: \\[ Y_{ij} = \\beta_{0}+\\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + \\epsilon_{ij}, \\] where \\(\\epsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\). Examples: Diet Expected Weight Random Deviation Litter 1, Mouse 1 A \\(\\beta_0\\) \\(\\epsilon_{11}\\) Litter 1, Mouse 3 B \\(\\beta_0 + \\beta_1\\) \\(\\epsilon_{13}\\) Litter 1, Mouse 5 C \\(\\beta_0 + \\beta_2\\) \\(\\epsilon_{15}\\) Litter 2, Mouse 1 A \\(\\beta_0\\) \\(\\epsilon_{21}\\) Litter 2, Mouse 3 B \\(\\beta_0 + \\beta_1\\) \\(\\epsilon_{23}\\) Litter 2, Mouse 5 C \\(\\beta_0 + \\beta_2\\) \\(\\epsilon_{25}\\) 2.2.2 Model Accounting For Correlation Let \\(Y_{ij}\\) denote the weight gain of mouse \\(j\\) in litter \\(i\\). \\(j=1,2,\\ldots, 6\\), \\(j=1,2,\\ldots, 6\\). We assume that: expected (or average) weight gain differs between diets individual litters vary from their expected weight randomly, according to normal distributions with constant variance \\(\\sigma^2_l\\) within each litter, individual mice vary from their expected weights randomly, according to normal distributions with constant variance \\(\\sigma^2\\) A model would have the form: \\[ Y_{ij} = \\beta_{0}+\\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + l_{i} + \\epsilon_{ij}, \\] where \\(l_i\\sim\\mathcal{N}(0, \\sigma^2_l)\\), and \\(\\epsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\). Diet Expected Weight Random Deviation Litter 1, Mouse 1 A \\(\\beta_0\\) \\(l_1 + \\epsilon_{11}\\) Litter 1, Mouse 3 B \\(\\beta_0 + \\beta_1\\) \\(l_1 + \\epsilon_{13}\\) Litter 1, Mouse 5 C \\(\\beta_0 + \\beta_2\\) \\(l_1 + \\epsilon_{15}\\) Litter 2, Mouse 1 A \\(\\beta_0\\) \\(l_2 + \\epsilon_{21}\\) Litter 2, Mouse 3 B \\(\\beta_0 + \\beta_1\\) \\(l_2 + \\epsilon_{23}\\) Litter 2, Mouse 5 C \\(\\beta_0 + \\beta_2\\) \\(l_2 + \\epsilon_{25}\\) 2.2.3 Questions of Interest The model \\[ Y_{ij} = \\beta_{0}+\\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + l_{i} + \\epsilon_{ij}, \\] where \\(l_i\\sim\\mathcal{N}(0, \\sigma^2_l)\\), and \\(\\epsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\) has 5 parameters: \\(\\beta_0\\) - average weight gain for mice on diet A. \\(\\beta_1\\) - difference in average weight gain for mice on diet B, compared to to diet A \\(\\beta_2\\) - difference in average weight gain for mice on diet C, compared to to diet A \\(\\sigma_l\\) - standard deviation in the distribution of differences between litters (i.e. variability explained by litter) \\(\\sigma\\) - standard deviation in the distribution of differences between individual mice in the same litter (i.e. unexplained variability) Thus, for a mouse in litter 1, the expected weight gain follows a normal distribution with mean \\(\\beta_0\\) and variance \\(\\sigma^2_l + \\sigma^2\\). For a mouse in litter 2, the expected weight gain follows a normal distribution with mean \\(\\beta_0 + \\beta_2\\) and variance \\(\\sigma^2_l + \\sigma^2\\) This is based on the fact that the sum of two independent normal random variables is normal, with mean equal to the sum of the means and variance equal to the sum of the variances. 2.2.4 Fixed and Random Effects In this case, we want to test for whether there are differences in weight gain between the diets. There is no reason to test for differences in weight gain between the litters. Differences between these specific litters of mice are not important to us. We're not interested in drawing conclusions about these specific mice. They're just a sample of participants, being used to investigate the diets. It's likely that we'll never acually see these specific litters of mice beyond this study. We still need to account for litter, though, because it helps explain, or account for, variability that would otherwise go unexplained. Variables for which we want to investigate differences or relationships are called fixed effects. We should build these into the &quot;expectation structure&quot; of the model, using \\(\\beta_j\\)'s. Variables that we are not interested in testing for differences or relationships between, but that we still want to include in our model in order to account for correlation and explain variability are called random effects. We should add these to the model as normally distributed error terms. Accounting for random effects allows us to accurately calculate standard errors associated with fixed effects. A model that involves both fixed and random effects is called a linear mixed effects model. 2.2.5 Fitting the Model in R To fit a linear mixed effects model in R, we use the lmer() command that is part of the lme4 package. It is also helpful to load the lmerTest package, in order to obtain p-values in the output. To add a random effect for a variable add (1 | variable_name) in the model. M_LME &lt;- lmer(data=mice, weight_gain~factor(diet) + (1 | factor(litter))) summary(M_LME) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: weight_gain ~ factor(diet) + (1 | factor(litter)) ## Data: mice ## ## REML criterion at convergence: -193.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2468 -0.7088 0.0247 0.6028 1.9280 ## ## Random effects: ## Groups Name Variance Std.Dev. ## factor(litter) (Intercept) 0.00374095 0.061163 ## Residual 0.00005267 0.007257 ## Number of obs: 36, groups: factor(litter), 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.057985 0.025058 5.046907 2.314 0.06807 . ## factor(diet)B -0.023942 0.002963 28.000000 -8.081 0.00000000846 *** ## factor(diet)C -0.010856 0.002963 28.000000 -3.664 0.00103 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fct()B ## factor(dt)B -0.059 ## factor(dt)C -0.059 0.500 Estimates and Interpretations \\(\\beta_0 = 0.058\\): We estimate that average weight gain for Diet A is 0.058 g. \\(\\beta_1 = -0.024\\): We estimate that mice on diet B gain 0.024 g. less than mice on diet A, on average. \\(\\beta_2 = -0.011\\): We estimate that mice on diet C gain 0.011 g. less than mice on diet A, on average. The &quot;Random effects&quot; table gives estimates of \\(\\sigma^2_l\\) (litters) and \\(\\sigma^2\\) (Residual). \\(\\sigma_l = 0.061\\): We estimate that the standard deviation in differences in weights between litters, after accounting for diet, is 0.061 g. \\(\\sigma = 0.007\\): We estimate that the standard deviation in differences in weights between mice within a litter, after accounting for diet, is 0.007 g. There is evidence of differences between the diets. There is more variability in weight between different litters than between mice in the same litter, after accounting for diet. 2.2.6 Why Not Fixed Effect for Litter? Why would a model like the following, which uses litter as an expanatory variable in Example 1, not be very useful? M_fixed_litter &lt;- lm(data=mice, weight_gain~ factor(diet) + factor(litter)) summary(M_fixed_litter) ## ## Call: ## lm(formula = weight_gain ~ factor(diet) + factor(litter), data = mice) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0164273 -0.0051744 0.0001942 0.0044281 0.0138701 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.046333 0.003421 -13.544 0.0000000000000815 *** ## factor(diet)B -0.023942 0.002963 -8.081 0.0000000084646521 *** ## factor(diet)C -0.010856 0.002963 -3.664 0.00103 ** ## factor(litter)2 0.126011 0.004190 30.075 &lt; 0.0000000000000002 *** ## factor(litter)3 0.061021 0.004190 14.564 0.0000000000000136 *** ## factor(litter)4 0.147712 0.004190 35.254 &lt; 0.0000000000000002 *** ## factor(litter)5 0.156366 0.004190 37.320 &lt; 0.0000000000000002 *** ## factor(litter)6 0.134801 0.004190 32.173 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.007257 on 28 degrees of freedom ## Multiple R-squared: 0.9874, Adjusted R-squared: 0.9843 ## F-statistic: 314.5 on 7 and 28 DF, p-value: &lt; 0.00000000000000022 We're now estimating 8 \\(\\beta&#39;s\\) instead of 3. Each time we estimate an additional parameter, we lose a degree of freedom, making estimates and predictions less precise. We don't care about differences between the litters, so \\(\\beta_3, \\beta_4, \\ldots, \\beta_7\\) are not useful. Imagine if we had many more litters. Things would get really messy, and unnecessarily so. If, for some reason, we really wanted to test for differences in weight gain between these specific litters of mice, then we would treat them as fixed effects, but it's hard to see why we would want to do that. 2.3 A Second Mice Experiment 2.3.1 Weight Gain in Mice: Experiment 2 Now consider a different structure of the mouse experiment. In this version of the experiment, the three diets were randomly assigned to six pregnant mice, so that two mice were assigned to each diet. Each of the six mice (dams), gave birth to six pups, creating six litters of six, as seen before. Researchers the observed the mean weight gain of the pups over a four week period. Now, each pup in a litter has necessarily been assigned to the same diet, since diets were assigned to the dams, before the pups were born. Experimental Design 2: Figure 2.2: Mouse Experiment Version 2 2.3.2 Mice Experiment 2 Data The first 15 rows of the dataset look like this: head(mice2,15) ## litter diet weight_gain ## 1 1 A -0.0352604099 ## 2 1 A -0.0474166604 ## 3 1 A -0.0545130532 ## 4 1 A -0.0517145212 ## 5 1 A -0.0481739132 ## 6 1 A -0.0505177486 ## 7 2 A 0.0758408263 ## 8 2 A 0.0701352749 ## 9 2 A 0.0885838352 ## 10 2 A 0.0727807107 ## 11 2 A 0.0826007925 ## 12 2 A 0.0785294998 ## 13 3 B 0.0002306143 ## 14 3 B -0.0163362762 ## 15 3 B -0.0146213462 2.3.3 A Naive Graphical Analysis for Experiment 2 Again, we'll temporarily ignore the fact that mice come from the same litter and treat all observations as independent. ggplot(data=mice2, aes(x=factor(diet), y=weight_gain)) + geom_point() + stat_summary(fun=&quot;mean&quot;, geom=&quot;line&quot;, aes(group=factor(1)))+ stat_summary(fun=&quot;mean&quot;, geom=&quot;point&quot;, color=&quot;red&quot;, size=2) 2.3.4 A More Informed Graphical Analysis for Experiment 2 Now, we'll account for the fact that mice in the same litter got the same diets. The plot below adds color to show litter. ggplot(data=mice2, aes(x=factor(diet), y=weight_gain, color=factor(litter))) + geom_point() Since diets were assigned to litters, not individual mice, it is the litters we should be comparing. When comparing diets, our observational units are litters, not individual mice. Since there are only 6 litters, our sample size is 6, rather than 36. Thus, the best graphical analysis would come from the following plot, which displays average weight for each litter: Litters &lt;- mice2 %&gt;% group_by(diet, litter)%&gt;% summarize(mean_weight_gain=mean(weight_gain), N=n()) head(Litters) ## # A tibble: 6 x 4 ## # Groups: diet [3] ## diet litter mean_weight_gain N ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 1 -0.0479 6 ## 2 A 2 0.0781 6 ## 3 B 3 -0.00791 6 ## 4 B 4 0.0788 6 ## 5 C 5 0.0994 6 ## 6 C 6 0.0779 6 ggplot(data=Litters, aes(x=factor(diet), y=mean_weight_gain, color=factor(litter))) + geom_point() 2.3.5 Table of Mean Weight By Diet for Experiment 2 mouse_groups &lt;- mice2 %&gt;% group_by(diet)%&gt;% summarize(Mean_Weight=mean(weight_gain), SD_Weight = sd(weight_gain), N=n()) mouse_groups ## # A tibble: 3 x 4 ## diet Mean_Weight SD_Weight N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 0.0151 0.0661 12 ## 2 B 0.0354 0.0457 12 ## 3 C 0.0887 0.0137 12 This standard deviations in this table pertain to variability between the 12 mice that got each diet (6 from one litter and 6 from another). These are not useful here, since diets were assigned to litters, not individual mice. 2.3.6 Table Comparing Litters Means by Diet for Experiment 2 We now look at means and standard deviations between litter means, using the average rate in each litter as the response variable. litter_groups &lt;- Litters %&gt;% group_by(diet)%&gt;% summarize(Mean_Weight=mean(mean_weight_gain), SD_Weight = sd(mean_weight_gain), N=n()) litter_groups ## # A tibble: 3 x 4 ## diet Mean_Weight SD_Weight N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 0.0151 0.0891 2 ## 2 B 0.0354 0.0613 2 ## 3 C 0.0887 0.0152 2 2.3.7 An Inappropriate Model for the Second Design An ordinarly linear least squares regression model fails to account for the fact that treatments were assigned to litters, not mice. It is based on the assumption that we have 36 independent mice (which is incorret). Such a model would have the form \\[ Y_{ij} = \\beta_{0}+\\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + \\epsilon_{ij}, \\] Output for such a model is shown below. M2_LLSR &lt;- lm(data=mice2, weight_gain~factor(diet)) summary(M2_LLSR) ## ## Call: ## lm(formula = weight_gain ~ factor(diet), data = mice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.069586 -0.041328 -0.005675 0.041915 0.073511 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01507 0.01359 1.109 0.275297 ## factor(diet)B 0.02036 0.01922 1.060 0.297007 ## factor(diet)C 0.07358 0.01922 3.829 0.000545 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.04707 on 33 degrees of freedom ## Multiple R-squared: 0.3215, Adjusted R-squared: 0.2804 ## F-statistic: 7.819 on 2 and 33 DF, p-value: 0.001662 2.3.8 A More Appropriate Model for Experiment 2 A linear mixed effects model with a random term for litter accounts for the fact that treatments were applied to litters, not individual mice. This model has the form: \\[ Y_{ij} = \\beta_{0}+\\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + l_{i} + \\epsilon_{ij}, \\] where \\(l_i\\sim\\mathcal{N}(0, \\sigma^2_l)\\), and \\(\\epsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\) Output for a model that accounts for correlation between mice in the same litter is shown. M2_LME &lt;- lmer(data=mice2, weight_gain~factor(diet) + (1 | factor(litter))) summary(M2_LME) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: weight_gain ~ factor(diet) + (1 | factor(litter)) ## Data: mice2 ## ## REML criterion at convergence: -206.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.33470 -0.59248 0.03505 0.59073 1.91089 ## ## Random effects: ## Groups Name Variance Std.Dev. ## factor(litter) (Intercept) 0.00396804 0.062992 ## Residual 0.00005093 0.007136 ## Number of obs: 36, groups: factor(litter), 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.01507 0.04459 3.00000 0.338 0.758 ## factor(diet)B 0.02036 0.06306 3.00000 0.323 0.768 ## factor(diet)C 0.07358 0.06306 3.00000 1.167 0.328 ## ## Correlation of Fixed Effects: ## (Intr) fct()B ## factor(dt)B -0.707 ## factor(dt)C -0.707 0.500 Estimates are the same Standard errors are larger, due mostly to the fact that our sample size is 6, instead of 36 after accounting for diet, standard deviation in weights between litters is estimated to be 0.0629 g. (estimate of \\(\\sigma_l\\)) after accounting for diet, standard deviation in weights between mice in the same litter is estimated to be 0.0071 g. (estimate of \\(\\sigma\\)) 2.3.9 Model for Litter Means Alternatively, we could fit a model, using the 6 litters as our observations, with the mean weight in each litter as the response variable. If we now let \\(Y_i\\) represent the mean weight in litter i, our model has the form: \\[ Y_{i} = \\beta_{0}+\\beta_{1}\\textrm{DietB}_{i} +\\beta_{2}\\textrm{DietC}_{i} + \\epsilon_{i}, \\] where \\(\\epsilon_{i} \\sim\\mathcal{N}(0, \\sigma^2)\\) Since it is reasonable to assume that litters are independent, we could use an ordinary LLSR model in this context. M2_Means &lt;- lm(data=Litters, mean_weight_gain~factor(diet)) summary(M2_Means) ## ## Call: ## lm(formula = mean_weight_gain ~ factor(diet), data = Litters) ## ## Residuals: ## 1 2 3 4 5 6 ## -0.06301 0.06301 -0.04335 0.04335 0.01078 -0.01078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01507 0.04459 0.338 0.758 ## factor(diet)B 0.02036 0.06306 0.323 0.768 ## factor(diet)C 0.07358 0.06306 1.167 0.328 ## ## Residual standard error: 0.06306 on 3 degrees of freedom ## Multiple R-squared: 0.3261, Adjusted R-squared: -0.1231 ## F-statistic: 0.7259 on 2 and 3 DF, p-value: 0.5532 Estimates, standard errors, and p-values are identical to the ones seen in the mixed-effects model. 2.3.10 Fixed Effect for Litter in Experiment 2 If we try to treat litter as a fixed effect in Experiment 2, we would not even be able to estimate all of the parameters. M2_fixed_litter &lt;- lm(data=mice2, weight_gain~ factor(diet) + factor(litter)) summary(M2_fixed_litter) ## ## Call: ## lm(formula = weight_gain ~ factor(diet) + factor(litter), data = mice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0166840 -0.0041608 0.0003311 0.0042513 0.0136135 ## ## Coefficients: (2 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.047933 0.002913 -16.453 &lt; 0.0000000000000002 *** ## factor(diet)B 0.126712 0.004120 30.755 &lt; 0.0000000000000002 *** ## factor(diet)C 0.125801 0.004120 30.533 &lt; 0.0000000000000002 *** ## factor(litter)2 0.126011 0.004120 30.585 &lt; 0.0000000000000002 *** ## factor(litter)3 -0.086691 0.004120 -21.041 &lt; 0.0000000000000002 *** ## factor(litter)4 NA NA NA NA ## factor(litter)5 0.021565 0.004120 5.234 0.000012 *** ## factor(litter)6 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.007136 on 30 degrees of freedom ## Multiple R-squared: 0.9858, Adjusted R-squared: 0.9835 ## F-statistic: 417.2 on 5 and 30 DF, p-value: &lt; 0.00000000000000022 2.4 A Multilevel Experiment 2.4.1 Experiment 2 Illustration In example 2, we saw that treatments (diets) were assigned to litters (dams), but measurements were taken on the individual mice (pups). 2.4.2 Experiment with Variables Assigned at Different Levels Now imagine qn experimemt with the following setup diets are still assigned to dams, prior to the birth of the pups, so all mice in the same litter get the same diet within each litter three mice are given nutritional supplements after their birth and the other three are not We want to study the effect of diet and supplement on weight gain. One treatment (diet) is assigned to litters, while the other (supplement) is assigned to individual mice. For the purpose of comparing diets, our observational units are 6 independent litters. For the purpose of comparing supplements, our observational units are 36 individual mice (who are not independent) An experiment where treatments are assigned at different levels is called a multilevel experiment level 1 observational units are litters, and level 1 treatment is diets level 2 observational units are mice, and level 2 treatment is supplement 2.4.3 Experiment 3 Data head(mice3,15) ## litter diet supplement weight_gain ## 1 1 A 1 -0.033260410 ## 2 1 A 0 -0.047416660 ## 3 1 A 1 -0.052513053 ## 4 1 A 0 -0.051714521 ## 5 1 A 1 -0.046173913 ## 6 1 A 0 -0.050517749 ## 7 2 A 1 0.077840826 ## 8 2 A 0 0.070135275 ## 9 2 A 1 0.090583835 ## 10 2 A 0 0.072780711 ## 11 2 A 1 0.084600792 ## 12 2 A 0 0.078529500 ## 13 3 B 1 0.002230614 ## 14 3 B 0 -0.016336276 ## 15 3 B 1 -0.012621346 2.4.4 Graphical Analysis for Experiment 3 We use color to represent litter, and shape to represent supplement. We'll use the argument position=position_jitterdodge() to stagger litters and supplement levels, which avoids overlap and makes the graph easier to read. ggplot(data=mice3, aes(x=factor(diet), y=weight_gain, color=factor(litter), shape=factor(supplement))) + geom_point(position=position_jitterdodge()) 2.4.5 An Inappropriate Model for the 3rd Design An ordinarly linear least squares regression model fails to account for the fact that treatments were assigned to litters, not mice. It is based on the assumption that we have 36 independent mice (which is incorret). The model has the form \\[ Y_{ij} = \\beta_{0} + \\alpha\\textrm{Supplement}_i + \\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + \\epsilon_{ij}, \\] where \\(\\epsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\) Output for such a model is shown below. M3_LLSR &lt;- lm(data=mice3, weight_gain~supplement + factor(diet)) summary(M3_LLSR) ## ## Call: ## lm(formula = weight_gain ~ supplement + factor(diet), data = mice3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.072685 -0.040982 -0.005618 0.040366 0.070412 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.011974 0.015895 0.753 0.456766 ## supplement 0.008198 0.015895 0.516 0.609538 ## factor(diet)B 0.020361 0.019467 1.046 0.303431 ## factor(diet)C 0.073578 0.019467 3.780 0.000648 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.04768 on 32 degrees of freedom ## Multiple R-squared: 0.3263, Adjusted R-squared: 0.2632 ## F-statistic: 5.167 on 3 and 32 DF, p-value: 0.005021 2.4.6 A More Appropriate Model for Experiment 3 We instead fit a linear mixed effects model. Since we want to study the effects of diet and supplement, we treat supplement and diet as fixed effects. Since we want to account for correlation due to mice being in the same litter, we treat litter as a random effect. Our model has the form \\[ Y_{ij} = \\beta_{0} + \\alpha\\textrm{Supplement}_i + \\beta_{1}\\textrm{DietB}_{ij} +\\beta_{2}\\textrm{DietC}_{ij} + l_{i} + \\epsilon_{ij}, \\] where \\(l_i\\sim\\mathcal{N}(0, \\sigma^2_l)\\), and \\(\\epsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\) Output for a model that accounts for correlation between mice in the same litter is shown. M3_LME &lt;- lmer(data=mice3, weight_gain ~ supplement + factor(diet) + (1 | factor(litter))) summary(M3_LME) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: weight_gain ~ supplement + factor(diet) + (1 | factor(litter)) ## Data: mice3 ## ## REML criterion at convergence: -203.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.12499 -0.76119 0.06055 0.60135 1.64980 ## ## Random effects: ## Groups Name Variance Std.Dev. ## factor(litter) (Intercept) 0.00396973 0.063006 ## Residual 0.00004076 0.006384 ## Number of obs: 36, groups: factor(litter), 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.011974 0.044603 3.003417 0.268 0.805735 ## supplement 0.008198 0.002128 29.000000 3.853 0.000596 *** ## factor(diet)B 0.020361 0.063060 3.000000 0.323 0.767980 ## factor(diet)C 0.073578 0.063060 3.000000 1.167 0.327609 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) spplmn fct()B ## supplement -0.024 ## factor(dt)B -0.707 0.000 ## factor(dt)C -0.707 0.000 0.500 Interpretations Interpretations for fixed effects are the same as in LLSR. We expect mice on the supplement to gain 0.008 g. more than mice not on the supplement, assuming they get the same diet. We expect mice in diet B to gain 0.02 g. more than mice on diet A, assuming supplement is held constant. We expect mice in diet C to gain 0.07 g. more than mice on diet A, assuming supplement is held constant After accounting for differences in diet and supplement, the standard deviation in weights between litters is estimated to be 0.063 g. (an estimate of \\(\\sigma_l\\)). After accounting for differences in diet and supplement, the standard deviation in weights between mice in the same litter is estimated to be 0.00638 g. (an estimate of \\(\\sigma\\)). 2.4.7 Comparison of LLSR and LME Models Compared to the incorrect LLSR model, when we use the linear mixed effects model: estimates for fixed effects supplement and diet do not change standard error for supplement is smaller - we get a more precise comparison for supplements because the model has accounted for variability due to differences in litters standard error for diet is larger - since diets were assigned to litters, our sample size is 6, not 36, so standard errors are higher The mixed effects model suggests evidence of differences due to supplement, but not evidence of differences due to diet. This is the opposite of the incorrect LLSR model. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
